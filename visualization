
import numpy as np
import matplotlib.pyplot as plt
from sklearn import svm, datasets
import os
import scipy.io
import numpy as np
import cv2
import matplotlib as mpl
import matplotlib.pyplot as plt
from IPython.display import display
import pandas as pd
from PIL import Image
from skimage.feature import hog
from skimage.color import rgb2grey
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler
from sklearn.decomposition import PCA
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier

from sklearn.metrics import roc_curve, auc
from PIL import ImageFilter
from skimage.feature import greycomatrix, greycoprops
from matplotlib.colors import ListedColormap

fname='E:/paper/dataset/dataset_liver_bmodes_steatosis_assessment_IJCARS.mat'
data = scipy.io.loadmat(fname)

label=[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 2, 1, 1, 2, 1,
        1, 1, 2, 2, 2, 1, 1, 1, 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 1, 2, 2, 1, 1,
        1, 1, 1, 1, 1, 2, 2, 1, 1]
def create_features(img):
    # flatten three channel color image
    color_features = img.flatten()
    # convert image to greyscale
    grey_image = rgb2grey(img)
    # get HOG features from greyscale image
    hog_features = hog(grey_image, block_norm='L2-Hys',
                       pixels_per_cell=(16, 16))
    
    g= greycomatrix(img, [5], [0.,np.pi/4], levels=256, 
                    normed=True, symmetric=True)
    
    contrast= greycoprops(g,'contrast')
    #print(np.shape(contrast))
    contrast_f= contrast.flatten()
    flat_features = contrast_f
    return flat_features

def create_feature_matrix():
    features_list = []
    
    for img_id in range(55):
        # load image
        img = data['data'][0][img_id][3][1]
        #blur_img = cv2.blur(img,(5,5))
        # get features for image
        image_features = create_features(img)
        features_list.append(image_features)
    # convert list of arrays into a matrix
    feature_matrix = np.array(features_list)
    return feature_matrix

feature_matrix = create_feature_matrix()

ss = StandardScaler()
# run this on our feature matrix
img_stand = ss.fit_transform(feature_matrix)

pca = PCA()
# use fit_transform to run PCA on our standardized matrix
img_pca = pca.fit_transform(img_stand)
# look at new shape
#print('PCA matrix shape is: ', img_pca.shape)
X = pd.DataFrame(img_pca)
y = pd.Series(label)

X_train, X_test, y_train, y_test = train_test_split(X,
                                                    y,
                                                    test_size=.35,
                                                    random_state=1234123)

X=np.array(X_train)
Y=np.array(y_train)

iris = datasets.load_iris()
#X = iris.data[:, :2]  # we only take the first two features. We could
                      # avoid this ugly slicing by using a two-dim dataset

#Y = iris.target
#X = X1[:, :2]

h = .02  # step size in the mesh

# we create an instance of SVM and fit out data.
clf = svm.SVC(kernel='linear')
clf.fit(X, Y)
# Plot the decision boundary. For that, we will assign a color to each
# point in the mesh [x_min, x_max]x[y_min, y_max].
x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
xx, yy = np.meshgrid(np.arange(x_min, x_max, h), np.arange(y_min, y_max, h))
Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])

# Put the result into a color plot
Z = Z.reshape(xx.shape)
plt.pcolormesh(xx, yy, Z, cmap=plt.cm.Paired)

# Plot also the training points
plt.scatter(X[:, 0], X[:, 1], c=Y, cmap=plt.cm.Paired, edgecolors='k')
plt.title('3-Class classification using Support Vector Machine with custom'
          ' kernel')
plt.axis('tight')
plt.show()
